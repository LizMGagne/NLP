{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "Liz McQuillan\n",
    "4/30/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Uses\n",
    "\n",
    "Given a large corpus of text, Topic Modeling is one way to get a general overview of the different topics within the text, the proportion of these themes, or even find hidden patterns within the corpus. This is different from rules-based approaches in text mining (like keyword searches), in that it's an unsupervised technique for finding linked groups of words (\"topics\") in a large corpus. \n",
    "\n",
    "Topics are generally defined as \"a repeating pattern of co-occuring terms\". Topic Models are used for clustering documents, feature selection, and information retrieval among other things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Methods\n",
    "\n",
    "There are a handful of techniques for getting topics from text, including Term Frequency-Inverse Document Frequency (TF-IDF), NonNegative Matrix Factorization (NMF), Latent Semantic Analysis (LSA), Heirarchical Dirichlet Process (HPD), and Latent Dirichlet Allocation (LDA). LDA is the most popular topic modeling technique, so that's what I'll focus on here. \n",
    "\n",
    "For LDA, Gensim is the premier Python Package. scikit-learn has some alternative algorithms, like NMF, but doesn't have LDA (LDA in scikit-learn = Linear Discriminant Analysis).\n",
    "\n",
    "LDA is a matrix factorization technique, and thus requires a document-term matrix as input. LDA takes a document-term matrix and tries to figure out which topics would create those documents based on the assumption that documents are produced from a bunch of topics which themselves are made up of words based on various probability distributions. \n",
    "\n",
    "The interim steps in this process include converting the document-term matrix into two matrices, a document-topics matric and a topic-terms matrix, which contain initial document/topic and topic/word distributions. Here's where LDA actually starts working. LDA aims to improve these matrices through a variety of sampling techniques. Basically, LDA iterates thorugh each word for each document to adjust the topic-word assignment (assuming all current topic/word assignments are correct) until a steady state is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import spacy\n",
    "import en_core_web_sm  # or any other model you downloaded via spacy download or pip\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data\n",
    "Let's pull in some text data to work with. \n",
    "\n",
    "We're going to use a subset of the 20 Newsgroups dataset, via Sci-Kit Learn. \n",
    "\n",
    "By using the Pandas package we can enforce a tabular structure on the data. This is especially helpful if you're used to working in SQL, SAS, or Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "df = pd.DataFrame([newsgroups.data, newsgroups.target.tolist()]).T\n",
    "df.columns = ['text', 'target']\n",
    "targets = pd.DataFrame( newsgroups.target_names)\n",
    "targets.columns=['title']\n",
    "news_data = pd.merge(df, targets, left_on='target', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data\n",
    "Then we'll do some basic pre-processing to clean the data\n",
    "\n",
    "See this page (https://github.com/LizMcQuillan/NLP/blob/master/NLP%20Pre-processing.ipynb) for a more thorough explaination of NLP pre-processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>[wondering, enlighten, car, saw, day, 2door, s...</td>\n",
       "      <td>[wonder, enlighten, car, see, day, 2door, spor...</td>\n",
       "      <td>[VERB, VERB, NOUN, VERB, NOUN, NUM, NOUN, NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I recently posted an article asking what kind ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>[recently, posted, article, asking, kind, rate...</td>\n",
       "      <td>[recently, post, article, ask, kind, rate, sin...</td>\n",
       "      <td>[ADV, VERB, NOUN, VERB, NOUN, NOUN, ADJ, NOUN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\\nIt depends on your priorities  A lot of peop...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>[depends, priorities, lot, people, higher, pri...</td>\n",
       "      <td>[depend, priority, lot, people, high, priority...</td>\n",
       "      <td>[VERB, NOUN, NOUN, NOUN, ADJ, NOUN, NOUN, NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>[excellent, automatic, found, subaru, legacy, ...</td>\n",
       "      <td>[excellent, automatic, find, subaru, legacy, s...</td>\n",
       "      <td>[ADJ, NOUN, VERB, PROPN, NOUN, VERB, NOUN, NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Ford and his automobile  I need information o...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>[Ford, automobile, need, information, Ford, pa...</td>\n",
       "      <td>[Ford, automobile, need, information, Ford, pa...</td>\n",
       "      <td>[PROPN, NOUN, VERB, NOUN, PROPN, ADV, ADJ, NOU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text target      title  \\\n",
       "0   I was wondering if anyone out there could enli...      7  rec.autos   \n",
       "17  I recently posted an article asking what kind ...      7  rec.autos   \n",
       "29  \\nIt depends on your priorities  A lot of peop...      7  rec.autos   \n",
       "56  an excellent automatic can be found in the sub...      7  rec.autos   \n",
       "64   Ford and his automobile  I need information o...      7  rec.autos   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [wondering, enlighten, car, saw, day, 2door, s...   \n",
       "17  [recently, posted, article, asking, kind, rate...   \n",
       "29  [depends, priorities, lot, people, higher, pri...   \n",
       "56  [excellent, automatic, found, subaru, legacy, ...   \n",
       "64  [Ford, automobile, need, information, Ford, pa...   \n",
       "\n",
       "                                               lemmas  \\\n",
       "0   [wonder, enlighten, car, see, day, 2door, spor...   \n",
       "17  [recently, post, article, ask, kind, rate, sin...   \n",
       "29  [depend, priority, lot, people, high, priority...   \n",
       "56  [excellent, automatic, find, subaru, legacy, s...   \n",
       "64  [Ford, automobile, need, information, Ford, pa...   \n",
       "\n",
       "                                                  pos  \n",
       "0   [VERB, VERB, NOUN, VERB, NOUN, NUM, NOUN, NOUN...  \n",
       "17  [ADV, VERB, NOUN, VERB, NOUN, NOUN, ADJ, NOUN,...  \n",
       "29  [VERB, NOUN, NOUN, NOUN, ADJ, NOUN, NOUN, NOUN...  \n",
       "56  [ADJ, NOUN, VERB, PROPN, NOUN, VERB, NOUN, NOU...  \n",
       "64  [PROPN, NOUN, VERB, NOUN, PROPN, ADV, ADJ, NOU...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "news_data['text'] = news_data['text'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "for doc in nlp.pipe(news_data['text'].astype('unicode').values, batch_size=100,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc if not n.is_punct and not n.is_stop and not n.is_space and not n.like_url])\n",
    "        lemma.append([n.lemma_ for n in doc if not n.is_punct and not n.is_stop and not n.is_space and not n.like_url])\n",
    "        pos.append([n.pos_ for n in doc if not n.is_punct and not n.is_stop and not n.is_space and not n.like_url])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "news_data['tokens'] = tokens\n",
    "news_data['lemmas'] = lemma \n",
    "news_data['pos'] = pos\n",
    "\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Corpus\n",
    "Now, let's take only the lemmas to build the dictionary and doc-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(lemma)\n",
    "\n",
    "# Converting corpus into Document-Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in lemma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Topic Model\n",
    "Now that we have the dictionary and doc-term matrix we can start building the LDA model. LDA requires the number of topics as an input. I've also specified chunksize (number of docs to be used in each training \"chunk\") and passes (the number of training passes).\n",
    "\n",
    "The LDA model might take a while to run. In a future notebook we'll talk about how to optimize runtime, automate hyperparameter tuning, and implement multiprocessing to speed this up considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, \n",
    "               num_topics=10, \n",
    "               random_state = 100, \n",
    "               chunksize = 100, \n",
    "               id2word = dictionary, \n",
    "               passes=10, \n",
    "               alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the Topics\n",
    "Using print_topics will print keywords for each topic and the relative weights of each word.\n",
    "\n",
    "##### Interpreting Topics\n",
    "In the case of our data Topic 0 is represented as ('0.072*\"key\" + 0.030*\"system\" + 0.021*\"need\"')\n",
    "\n",
    "This means the top words that contribute to the topic are \"not\", \"people\", and \"know\" and the weights represent the importance of each word. LDA requires a good deal of interpretation - it will not label the topics with a word or phrase, so it's up to the analyst to determine how to label each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.025*\"not\" + 0.014*\"people\" + 0.012*\"know\"'),\n",
       " (1, '0.026*\"happy\" + 0.010*\"suck\" + 0.006*\"selection\"'),\n",
       " (2, '0.078*\"Q\" + 0.049*\"MR\" + 0.041*\"STEPHANOPOULOS\"'),\n",
       " (3, '0.091*\"anonymous\" + 0.049*\"process\" + 0.028*\"archive\"'),\n",
       " (4, '0.015*\"tv\" + 0.014*\"reverse\" + 0.013*\"select\"'),\n",
       " (5, '0.031*\"1\" + 0.022*\"2\" + 0.015*\"3\"'),\n",
       " (6, '0.000*\"Rofekamp\" + 0.000*\"Notre\" + 0.000*\"films\"'),\n",
       " (7, '0.020*\"key\" + 0.011*\"use\" + 0.010*\"encryption\"'),\n",
       " (8, '0.130*\"government\" + 0.044*\"God\" + 0.033*\"device\"'),\n",
       " (9, '0.061*\"criminal\" + 0.018*\"variable\" + 0.017*\"publication\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=10, num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Optimal Number of Topics\n",
    "\n",
    "There's some disagreement among data scientists about what the best number of topics even means - is it coherence? comprehensiveness? something else? Personally, I err on the side of interpretability and meaningfulness - I don't want the same words repeated over and over throughout the topics. In practice this means building a handful of models with various k values and picking the one with the highest coherence score. The coherence score for our model is ~0.4 here - not ideal, but not terrible. It's a bit of a balancing act getting a \"good\" coehrence score, while maintaining an aceptable level of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4262870033224271\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=lemma, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda) #higher is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the LDA model are completely dependent on the data used (garbage in = garbage out) and the parameter choices we make. Since doc-term matrices are typically sparse, dimensionailty reduction may improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Filter\n",
    "Since terms which appear less often in the corpus are also less likely to appear in the results, the lowest frequency terms can be excluded. Some basic exploratory analysis of term frequencies is required to pinpoint an appropriate frequency threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Filter\n",
    "Earlier in this code some types of strings were filtered out (stop words, numbers, etc). Depending on the data being analyzed it may improve the model's accuracy to strip out further types of words. Whether these are additional filler words (i.e. \"within\", \"may\", etc) or some other words which occur in ways that render them meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Hyperparameter Optimization\n",
    "\n",
    "When building a model like LDA for use in production, it's necessary to automate much of the modeling process. This includes hyperparamers like alpha, beta, and the number of topics. In this notebook we used Gensim's 'auto' arg for our alpha hyperparameter, but there are better ways to optimize this that are outside the scope of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Pooling\n",
    "There's research to support creating macro-documents for LDA training might increase the accuracy and/or usability of topics by enriching the content in each document (http://users.cecs.anu.edu.au/~ssanner/Papers/sigir13.pdf). However, the documents being used here are quite long (average ~700 words) and assumedy have sufficient co-occurance of terms within each document to be used for training without aggregation. If the documents being analyzed were shorter (like Tweets, sms texts, and the like) it may be worthwhile to aggregate at some level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Documents to Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contribution</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4349</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>2door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4451</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6864</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5117</td>\n",
       "      <td>key, use, encryption, system, chip, number, DB...</td>\n",
       "      <td>Bricklin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4117</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contribution  \\\n",
       "0            0             0.0                   0.4349   \n",
       "1            1             0.0                   0.4451   \n",
       "2            2             0.0                   0.6864   \n",
       "3            3             7.0                   0.5117   \n",
       "4            4             0.0                   0.4117   \n",
       "\n",
       "                                            Keywords      Text  \n",
       "0  not, people, know, think, say, right, go, s, l...     2door  \n",
       "1  not, people, know, think, say, right, go, s, l...        60  \n",
       "2  not, people, know, think, say, right, go, s, l...        70  \n",
       "3  key, use, encryption, system, chip, number, DB...  Bricklin  \n",
       "4  not, people, know, think, say, right, go, s, l...  addition  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=ldamodel, corpus=lemma, texts=dictionary):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get dominant topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the dominant topic, percent Contribution and keywords for each doc\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=doc_term_matrix, texts=dictionary)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contribution', 'Keywords', 'Text']\n",
    "\n",
    "# Print the first 5 rows\n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Document That's Representitive of Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contribution</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8589</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>speedomete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6828</td>\n",
       "      <td>happy, suck, selection, plate, Wednesday, pain...</td>\n",
       "      <td>surrounding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>Q, MR, STEPHANOPOULOS, release, search, win, f...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8797</td>\n",
       "      <td>1, 2, 3, 4, 5, proposal, Security, April, Univ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>key, use, encryption, system, chip, number, DB...</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contribution  \\\n",
       "0        0.0                   0.8589   \n",
       "1        1.0                   0.6828   \n",
       "2        2.0                   0.9524   \n",
       "3        5.0                   0.8797   \n",
       "4        7.0                   0.8412   \n",
       "\n",
       "                                            Keywords         Text  \n",
       "0  not, people, know, think, say, right, go, s, l...   speedomete  \n",
       "1  happy, suck, selection, plate, Wednesday, pain...  surrounding  \n",
       "2  Q, MR, STEPHANOPOULOS, release, search, win, f...            B  \n",
       "3  1, 2, 3, 4, 5, proposal, Security, April, Univ...            M  \n",
       "4  key, use, encryption, system, chip, number, DB...          740  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contribution\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Print top 5 rows\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Distribution of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>7187.0</td>\n",
       "      <td>0.6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>key, use, encryption, system, chip, number, DB...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>not, people, know, think, say, right, go, s, l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic                                     Topic_Keywords  \\\n",
       "0.0             0.0  not, people, know, think, say, right, go, s, l...   \n",
       "1.0             0.0  not, people, know, think, say, right, go, s, l...   \n",
       "2.0             0.0  not, people, know, think, say, right, go, s, l...   \n",
       "3.0             7.0  key, use, encryption, system, chip, number, DB...   \n",
       "4.0             0.0  not, people, know, think, say, right, go, s, l...   \n",
       "\n",
       "     Num_Documents  Perc_Documents  \n",
       "0.0         7187.0          0.6352  \n",
       "1.0            1.0          0.0001  \n",
       "2.0           10.0          0.0009  \n",
       "3.0            NaN             NaN  \n",
       "4.0            NaN             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Format\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Print top 5 rows\n",
    "df_dominant_topics.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
