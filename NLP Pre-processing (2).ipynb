{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pre-Processing\n",
    "Liz Gagne  \n",
    "04/05/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Purpose of NLP Pre-Processing\n",
    "Natural language processing (NLP) is often thought of as one of the main areas in Artificial Intellegence.  NLP techniques are at the core of AI-based products we use every day - chat bots, Google Translate, article summarizers, and the like.  However, NLP actually sits at the crossroads of AI/CS and computational linguistics - it's applications are more widespread than the obvious applications listed above.  NLP techniques allow us to derive things as complex as sentiment from text data, or to find patterns in text for any number of applications (fraud detection, topic segmentation, etc).\n",
    "\n",
    "NLP is characterized as a difficult problem in computer science, due mostly to the ambiguity of human language. Human speech is seldom precise or direct.  Understanding natural language means you need to understand the concepts beneath the words, how they go together, and how the words/order/concepts come together to create meaning.  \n",
    "\n",
    "Before embarking on any NLP techniques (i.e. Sentiment Analysis) we need to make sure the text data is in the proper format.\n",
    "If not, the text won't be accepted into any models or processes.  Transforming your text data into something that an algorithm is able to ingest can be complicated, and it's helpful to have a solid grasp on the text data you're working with.  Generally, there are four stages within NLP pre-processing:\n",
    "- ##### Cleaning  \n",
    "Just like with non-text data, cleaning involves excluding the irrelevant or corrupt data points.  In NLP, this typically consists of removing stop words, punctuation, and other extraneous text.  Other cleaning tasks might involve dealing with capitalization rules, or other non-alphanumeric characters.\n",
    "- ##### Annotation  \n",
    "Typically annotation include things like parts-of-speech (POS) tagging, and is generally thought of as the application of a scheme to text data.\n",
    "- ##### Normalization  \n",
    "The translation or mapping or text within the scheme through Stemming, Lemmatization, or another method of standardization.\n",
    "- ##### Analysis  \n",
    "Applying basic statistical techniques to manipulate the data for more in depth analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Methods\n",
    "There are a lot methods and tools available for pre-processing text data. This article is meant to give you a starting point, and is not an exhaustive list of all the options available. Like with all data analysis, the analyst must understand the drawbacks and best uses of each technique and choose a method appropriate for the given dataset. The two main Python packages for NLP are `spaCy` and `NLTK` - both have pros and cons.  `NLTK` is highly customizable, but wasn't built to be quick and simple.  `spaCy`, on the other hand, was designed specifically with efficiency in mind and as such is quick and easy to use.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy.en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-21780ee33474>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#from spacy.en import English\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_sets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m  \u001b[1;31m# or any other model you downloaded via spacy download or pip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy.en'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "from spacy.en.word_sets import STOP_WORDS\n",
    "import en_core_web_sm  # or any other model you downloaded via spacy download or pip\n",
    "nlp = en_core_web_sm.load()\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull in some TIP data to work with.  \n",
    "By using pandas we can maintain the tabular structure of the data. This is especially helpful if you're used to working in SQL or SAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection to all dbs\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 11 for SQL Server};SERVER=ES11vADOSQL006;DATABASE=master;Trusted_Connection=yes;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from APPRTIP\n",
    "#Create an additional column with all text concatenated\n",
    "sql3 = \"\"\"\n",
    "SELECT EmployeeID, FiscalYear, TIPImprovementPlan1, TIPActionPlan, TIPTimelinePlan, TIPSupportPlan, TIPAssessmentPlan,(TIPImprovementPlan1 + ' ' + TIPActionPlan + ' ' + TIPTimeLinePlan + ' ' + TIPSupportPlan + ' ' + TIPAssessmentPlan) as TIP_all_txt\n",
    "FROM [APPR_EXT].[dbo].[APPRTIP]\n",
    "where IsSubmitted = 'Y' and TIPEndedAppeal = 'N'\n",
    "\"\"\"\n",
    "APPRTIP = pd.io.sql.read_sql(sql3, cnxn) #assign the SQL query to a pandas dataframe called APPRTIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the first 5 rows of data to make sure our dataframe looks like we expected it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>TIPImprovementPlan1</th>\n",
       "      <th>TIPActionPlan</th>\n",
       "      <th>TIPTimelinePlan</th>\n",
       "      <th>TIPSupportPlan</th>\n",
       "      <th>TIPAssessmentPlan</th>\n",
       "      <th>TIP_all_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0469849</td>\n",
       "      <td>2015</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>1.\\tFor Developing strategies for consistently...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1)\\tA Ramapo consultant will support you with ...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0469849</td>\n",
       "      <td>2017</td>\n",
       "      <td>1E: Designing Coherent Instruction: Design les...</td>\n",
       "      <td>1E: Designing Coherent Instruction:\\r\\n\\t• Des...</td>\n",
       "      <td>See above</td>\n",
       "      <td>1) You will schedule inter-visitations to obse...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>1E: Designing Coherent Instruction: Design les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0964702</td>\n",
       "      <td>2015</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>1.\\tFor Developing strategies for consistently...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1) Work with Deborah Flaum, PS15 Math coach, t...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1408117</td>\n",
       "      <td>2016</td>\n",
       "      <td>1)\\tDesigning coherent instruction by differen...</td>\n",
       "      <td>1.\\tFor Designing coherent instruction by diff...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1) A Gifted and Talented coach will support yo...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>1)\\tDesigning coherent instruction by differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1418900</td>\n",
       "      <td>2015</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>1.\\tFor Developing strategies for consistently...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1)\\tA Ramapo consultant will support you with ...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EmployeeID  FiscalYear                                TIPImprovementPlan1  \\\n",
       "0    0469849        2015  1) Developing strategies for consistently moni...   \n",
       "1    0469849        2017  1E: Designing Coherent Instruction: Design les...   \n",
       "2    0964702        2015  1) Developing strategies for consistently moni...   \n",
       "3    1408117        2016  1)\\tDesigning coherent instruction by differen...   \n",
       "4    1418900        2015  1) Developing strategies for consistently moni...   \n",
       "\n",
       "                                       TIPActionPlan  \\\n",
       "0  1.\\tFor Developing strategies for consistently...   \n",
       "1  1E: Designing Coherent Instruction:\\r\\n\\t• Des...   \n",
       "2  1.\\tFor Developing strategies for consistently...   \n",
       "3  1.\\tFor Designing coherent instruction by diff...   \n",
       "4  1.\\tFor Developing strategies for consistently...   \n",
       "\n",
       "                                     TIPTimelinePlan  \\\n",
       "0  Refer to the timelines included at the end of ...   \n",
       "1                                          See above   \n",
       "2  Refer to the timelines included at the end of ...   \n",
       "3  Refer to the timelines included at the end of ...   \n",
       "4  Refer to the timelines included at the end of ...   \n",
       "\n",
       "                                      TIPSupportPlan  \\\n",
       "0  1)\\tA Ramapo consultant will support you with ...   \n",
       "1  1) You will schedule inter-visitations to obse...   \n",
       "2  1) Work with Deborah Flaum, PS15 Math coach, t...   \n",
       "3  1) A Gifted and Talented coach will support yo...   \n",
       "4  1)\\tA Ramapo consultant will support you with ...   \n",
       "\n",
       "                                   TIPAssessmentPlan  \\\n",
       "0  In our second and third meetings, we will revi...   \n",
       "1  In our second and third meetings, we will revi...   \n",
       "2  In our second and third meetings, we will revi...   \n",
       "3  In our second and third meetings, we will revi...   \n",
       "4  In our second and third meetings, we will revi...   \n",
       "\n",
       "                                         TIP_all_txt  \n",
       "0  1) Developing strategies for consistently moni...  \n",
       "1  1E: Designing Coherent Instruction: Design les...  \n",
       "2  1) Developing strategies for consistently moni...  \n",
       "3  1)\\tDesigning coherent instruction by differen...  \n",
       "4  1) Developing strategies for consistently moni...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPRTIP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization\n",
    "Text data typically contains a various capitalizations - the beginning of sentences, proper nouns, etc.  The most common approach is to reduce everything to lower case, though this can sometimes affect the fidelity of your data - changing something like \"US\" to \"us\" can alter the meaning or affect how the string is tagged (this is where that inside knowledge of the data you're working with and the analysis you're running comes in handy).\n",
    "  \n",
    "### Stop Words  \n",
    "Most words within text data are connectors, which do little to show the subject, object, or other information within a sentence. Depending on the type of analysis you're running, excluding these stop words is a necessary step.  Stop words are equivilant to noise within the data.  There are pre-fab stop words lists out there, which can be used as is or altered to fit your particular data.\n",
    "  \n",
    "### Tokenization  \n",
    "Tokenization is used for splitting sentences into individual words and/or splitting paragraphs into sentences. Splitting sentences into individual words and punctuation is most often done by splitting across white space or punctuation. This might cause problems when you're working with abbreviations, possessives, or proper nouns that use puntuation (like O'Brien or Sackville-West).  Splitting paragraphs into sentences accurately is equally challenging, largely due to the ambiguity of puntuation in the English language. The period alone can be used to denote the end of a sentence, an abbreviation, or be included in an email address. To accurately identify the boundaries of sentences a pre-trained algorithm, like NLTK's Punkt Models, should be used.\n",
    "\n",
    "### Parts of Speech Tagging\n",
    "Parts of Speech (POS) tags are useful for understanding the meaning of a sentence, or identifying speech patterns in text. POS tagging typically entails looking at the neighboring words using either a stochastic or rule absed method.  \n",
    "\n",
    "### Stemming\n",
    "Stemming is a process where words are reduced to their root, removing whatever inflextion is present.  This is done by removing unnecessary characters, usually the suffix. There are a variety of models available for stemming, including Porter and Snowball. The results can be used to identify relationships and commonalities across data. The main drawback to stemming is that sometimes words are overstemmed to the point of uselessness. This happens when words are structurally similar but have vastly different meanings (i.e. \"universe\" and \"university\" both stem to \"univers\"). \n",
    "\n",
    "### Lemmatization\n",
    "Lemmazation is an alternative to stemming, that (at least for the NYCDOE text data that I work with) gets better results. Lemmazation is a more intensive process involving POS tags, which is often more accurate than stemming.  This increased accuracy comes at a slight time cost, so depending on your dataset and what's you're looking to extract from the text, consider what trade off is acceptable for you.  Generally, stemming is more appropriate for text queries whereas lemmazatiztion is a better choice when trying to determine sentiment.\n",
    "\n",
    "### Word Counts \n",
    "One of the more basic, but still powerful, tools for feature engineering is to calculate word, sentence, punctuation, and keyword counts. Again, this is where that knowledge of your data will serve you well - you can create your own list of keywords and then calculate the count of those specific words to store as a feature.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "While this is definitely not an exhaustive list of pre-proccessing techniques, preparing raw text data for analysis is a complicated process which requires the analyst to choose the optimal tools given both the data and the question being asked. Packages like `spaCy` and `NLTK` offer some great off the shelf funtions, though you may need to manually alter the default parameters or lists for best results. Once you've prepped your data you can go on to apply a variety of machine learning techniques depending on what the questions you're asking in regard to the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easiest to process the text if it's not in the dataframe.  However, ultimately we will appreciate having the dataframe structure.  So let's pull the text out of the dataframe, do some processing, and then stick it back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SchoolDBN</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>TIPImprovementPlan1</th>\n",
       "      <th>TIPActionPlan</th>\n",
       "      <th>TIPTimeLinePlan</th>\n",
       "      <th>TIPSupportPlan</th>\n",
       "      <th>TIPAssessmentPlan</th>\n",
       "      <th>TIPMeeting1</th>\n",
       "      <th>TIPMeeting2</th>\n",
       "      <th>...</th>\n",
       "      <th>UpdateDate</th>\n",
       "      <th>SubmitUserID</th>\n",
       "      <th>SubmitUserRole</th>\n",
       "      <th>SubmitDate</th>\n",
       "      <th>TIPImprovementPlan2</th>\n",
       "      <th>TIPImprovementPlan3</th>\n",
       "      <th>TIP_all_txt</th>\n",
       "      <th>s_tokens_IP</th>\n",
       "      <th>s_lemmas_IP</th>\n",
       "      <th>s_pos_IP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01M015</td>\n",
       "      <td>0469849</td>\n",
       "      <td>2015</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>1.\\tFor Developing strategies for consistently...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1)\\tA Ramapo consultant will support you with ...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>2014-09-12</td>\n",
       "      <td>2015-01-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-09-17 12:26:14.157</td>\n",
       "      <td>isanchez11</td>\n",
       "      <td>Principal</td>\n",
       "      <td>2014-09-15 22:40:25.483</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01M015</td>\n",
       "      <td>0469849</td>\n",
       "      <td>2017</td>\n",
       "      <td>1E: Designing Coherent Instruction: Design les...</td>\n",
       "      <td>1E: Designing Coherent Instruction:\\r\\n\\t• Des...</td>\n",
       "      <td>See above</td>\n",
       "      <td>1) You will schedule inter-visitations to obse...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>2016-09-16</td>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09-22 10:40:52.870</td>\n",
       "      <td>isanchez11</td>\n",
       "      <td>Principal</td>\n",
       "      <td>2016-09-22 10:33:39.267</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1E: Designing Coherent Instruction: Design les...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01M015</td>\n",
       "      <td>0964702</td>\n",
       "      <td>2015</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>1.\\tFor Developing strategies for consistently...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1) Work with Deborah Flaum, PS15 Math coach, t...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>2014-09-12</td>\n",
       "      <td>2015-01-21</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-09-16 08:07:24.520</td>\n",
       "      <td>isanchez11</td>\n",
       "      <td>Principal</td>\n",
       "      <td>2014-09-14 12:05:23.447</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01M015</td>\n",
       "      <td>1408117</td>\n",
       "      <td>2016</td>\n",
       "      <td>1)\\tDesigning coherent instruction by differen...</td>\n",
       "      <td>1.\\tFor Designing coherent instruction by diff...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1) A Gifted and Talented coach will support yo...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>2015-09-21</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-06-23 15:17:46.090</td>\n",
       "      <td>isanchez11</td>\n",
       "      <td>Principal</td>\n",
       "      <td>2015-09-28 11:27:59.373</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1)\\tDesigning coherent instruction by differen...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01M015</td>\n",
       "      <td>1418900</td>\n",
       "      <td>2015</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>1.\\tFor Developing strategies for consistently...</td>\n",
       "      <td>Refer to the timelines included at the end of ...</td>\n",
       "      <td>1)\\tA Ramapo consultant will support you with ...</td>\n",
       "      <td>In our second and third meetings, we will revi...</td>\n",
       "      <td>2014-09-17</td>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-09-17 12:51:48.293</td>\n",
       "      <td>isanchez11</td>\n",
       "      <td>Principal</td>\n",
       "      <td>2014-09-17 12:19:46.157</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1) Developing strategies for consistently moni...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  SchoolDBN EmployeeID  FiscalYear  \\\n",
       "0    01M015    0469849        2015   \n",
       "1    01M015    0469849        2017   \n",
       "2    01M015    0964702        2015   \n",
       "3    01M015    1408117        2016   \n",
       "4    01M015    1418900        2015   \n",
       "\n",
       "                                 TIPImprovementPlan1  \\\n",
       "0  1) Developing strategies for consistently moni...   \n",
       "1  1E: Designing Coherent Instruction: Design les...   \n",
       "2  1) Developing strategies for consistently moni...   \n",
       "3  1)\\tDesigning coherent instruction by differen...   \n",
       "4  1) Developing strategies for consistently moni...   \n",
       "\n",
       "                                       TIPActionPlan  \\\n",
       "0  1.\\tFor Developing strategies for consistently...   \n",
       "1  1E: Designing Coherent Instruction:\\r\\n\\t• Des...   \n",
       "2  1.\\tFor Developing strategies for consistently...   \n",
       "3  1.\\tFor Designing coherent instruction by diff...   \n",
       "4  1.\\tFor Developing strategies for consistently...   \n",
       "\n",
       "                                     TIPTimeLinePlan  \\\n",
       "0  Refer to the timelines included at the end of ...   \n",
       "1                                          See above   \n",
       "2  Refer to the timelines included at the end of ...   \n",
       "3  Refer to the timelines included at the end of ...   \n",
       "4  Refer to the timelines included at the end of ...   \n",
       "\n",
       "                                      TIPSupportPlan  \\\n",
       "0  1)\\tA Ramapo consultant will support you with ...   \n",
       "1  1) You will schedule inter-visitations to obse...   \n",
       "2  1) Work with Deborah Flaum, PS15 Math coach, t...   \n",
       "3  1) A Gifted and Talented coach will support yo...   \n",
       "4  1)\\tA Ramapo consultant will support you with ...   \n",
       "\n",
       "                                   TIPAssessmentPlan TIPMeeting1 TIPMeeting2  \\\n",
       "0  In our second and third meetings, we will revi...  2014-09-12  2015-01-22   \n",
       "1  In our second and third meetings, we will revi...  2016-09-16  2017-01-12   \n",
       "2  In our second and third meetings, we will revi...  2014-09-12  2015-01-21   \n",
       "3  In our second and third meetings, we will revi...  2015-09-21  2016-04-22   \n",
       "4  In our second and third meetings, we will revi...  2014-09-17  2015-01-16   \n",
       "\n",
       "    ...                 UpdateDate SubmitUserID SubmitUserRole  \\\n",
       "0   ...    2014-09-17 12:26:14.157   isanchez11      Principal   \n",
       "1   ...    2016-09-22 10:40:52.870   isanchez11      Principal   \n",
       "2   ...    2014-09-16 08:07:24.520   isanchez11      Principal   \n",
       "3   ...    2016-06-23 15:17:46.090   isanchez11      Principal   \n",
       "4   ...    2014-09-17 12:51:48.293   isanchez11      Principal   \n",
       "\n",
       "               SubmitDate TIPImprovementPlan2 TIPImprovementPlan3  \\\n",
       "0 2014-09-15 22:40:25.483                None                None   \n",
       "1 2016-09-22 10:33:39.267                None                None   \n",
       "2 2014-09-14 12:05:23.447                None                None   \n",
       "3 2015-09-28 11:27:59.373                None                None   \n",
       "4 2014-09-17 12:19:46.157                None                None   \n",
       "\n",
       "                                         TIP_all_txt s_tokens_IP s_lemmas_IP  \\\n",
       "0  1) Developing strategies for consistently moni...        None        None   \n",
       "1  1E: Designing Coherent Instruction: Design les...        None        None   \n",
       "2  1) Developing strategies for consistently moni...        None        None   \n",
       "3  1)\\tDesigning coherent instruction by differen...        None        None   \n",
       "4  1) Developing strategies for consistently moni...        None        None   \n",
       "\n",
       "  s_pos_IP  \n",
       "0     None  \n",
       "1     None  \n",
       "2     None  \n",
       "3     None  \n",
       "4     None  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, let's create 3 empty lists.  This is where we'll put the processed data for holding until we merge it back in with the dataframe.\n",
    "\n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "#Next we push our text through the nlp pipe\n",
    "\n",
    "for doc in nlp.pipe(APPRTIP['TIP_all_txt'].astype('unicode').values, batch_size=9845,\n",
    "                        n_threads=3):\n",
    "    \n",
    "#Here we're filling in our empty lists with the text, if it meets our set conditions\n",
    "#Basically we're saying if the word is not punctution and not a stop word and not extra whitespace then \n",
    "\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc if not n.is_punct and not n.is_stop and not n.is_space])\n",
    "        lemma.append([n.lemma_ for n in doc if not n.is_punct and not n.is_stop and not n.is_space])\n",
    "        pos.append([n.pos_ for n in doc if not n.is_punct and not n.is_stop and not n.is_space])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "#Now we create new columns in our datafram and populate them with the lists        \n",
    "APPRTIP['tip_tokens'] = tokens\n",
    "APPRTIP['tip_lemmas'] = lemma \n",
    "APPRTIP['tip_pos'] = pos\n",
    "\n",
    "APPRTIP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
