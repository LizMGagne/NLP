{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pre-processing\n",
    "Before embarking on any NLP techniques (i.e. Sentiment Analysis, etc.) we need to make sure the text data is in the proper format.\n",
    "If not, the text won't be accepted into any models or processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must import some libraries\n",
    "spaCy will be the main library we use for all NLP actions.  spaCy is efficient, and easy to use.  \n",
    "NLTK has some useful features as well, but isn't built for quick, simple nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download()\n",
    "import en_core_web_sm  # or any other model you downloaded via spacy download or pip\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you'll need to build your dataframe\n",
    "This is easily done using the \"Connect to SQL Server\" and \"Use SQL\" code snippets replicated below and available here: \\\\es00cifs00\\users$\\egagne\\WINNT\\System\\Desktop\\Python Code Snippets\n",
    "\n",
    "For this example we'll use the APPRTIP table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign server and db\n",
    "server = 'ES11vADOSQL006'\n",
    "db = 'master'\n",
    "\n",
    "# Create the connection to all dbs\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 11 for SQL Server};SERVER=ES11vADOSQL006;DATABASE=master;Trusted_Connection=yes;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from APPRTIP\n",
    "#Create an additional column will all text concatenated\n",
    "#Filter on IsSubmitted = 'Y' and TIPEndedAppeal = 'N'\n",
    "sql3 = \"\"\"\n",
    "\n",
    "SELECT *\n",
    ",(TIPImprovementPlan1 + ' ' + TIPActionPlan + ' ' + TIPTimeLinePlan + ' ' + TIPSupportPlan + ' ' + TIPAssessmentPlan) as TIP_all_txt\n",
    "FROM [APPR_EXT].[dbo].[APPRTIP]\n",
    "where IsSubmitted = 'Y' and TIPEndedAppeal = 'N'\n",
    "\n",
    "\"\"\"\n",
    "TIP_with_rats = pd.io.sql.read_sql(sql3, cnxn)\n",
    "TIP_with_rats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easiest to process the text if it's not in the dataframe.  However, ultimately we will appreciate having the dataframe structure.  So let's pull the text out of the dataframe, do some processing, and then stick it back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's create 3 empty lists.  This is where we'll put the processed data for holding until we merge it back in with the dataframe.\n",
    "\n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "#Next we push our text through the nlp pipe\n",
    "\n",
    "for doc in nlp.pipe(TIP_with_rats['TIPImprovementPlan1'].astype('unicode').values, batch_size=9845,\n",
    "                        n_threads=3):\n",
    "    \n",
    "#Here we're filling in our empty lists with the text, if it meets our set conditions\n",
    "#Basically we're saying if the word is not punctution and not a stop word and not extra whitespace then \n",
    "\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc if not n.is_punct and not n.is_stop and not n.is_space])\n",
    "        lemma.append([n.lemma_ for n in doc if not n.is_punct and not n.is_stop and not n.is_space])\n",
    "        pos.append([n.pos_ for n in doc if not n.is_punct and not n.is_stop and not n.is_space])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "#Now we create new columns in our datafram and populate them with the lists        \n",
    "TIP_with_rats['s_tokens_IP'] = tokens\n",
    "TIP_with_rats['s_lemmas_IP'] = lemma \n",
    "TIP_with_rats['s_pos_IP'] = pos\n",
    "\n",
    "TIP_with_rats.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
